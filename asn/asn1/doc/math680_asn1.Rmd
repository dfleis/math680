---
title: "MATH 680: Assignment 1"
author: "David Fleischer -- 260396047"
date: "Last Update: `r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# check what to do for the OLS/lambda = 0 case for p > n (Q4)
# remove longwinded answers (expected values/variance estimates... no need to use SVD)

# Question 1

From our definitions of $\tilde X$ and $\tilde Y$
\begin{align*}
  \tilde X &= X_{-1} - {\bf 1}_n \bar x^T \\
  \tilde Y &= Y - {\bf 1}_n^T \bar Y, 
\end{align*}

we find
\begin{align*}
  \hat\beta_{-1} &= \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert \tilde Y - \tilde X \beta \rVert^2_2 \\
  &= \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert Y - {\bf 1}_n\bar Y - \left(X_{-1} - {\bf 1}_n \bar x^T\right) \beta_{-1} \rVert^2_2 \\
  &= \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert Y - X_{-1}\beta_{-1} - {\bf 1}_n  \left( \bar Y - \bar x^T \beta_{-1} \right) \rVert^2_2 \\
  &= \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert Y - X_{-1}\beta_{-1} - {\bf 1}_n \beta_1 \rVert^2_2 \quad \text{(by definition of $\beta_1$ above)} \\ 
  &= \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert Y - \left[{\bf 1}_n,\, X_{-1} \right]\,\left[\beta_1,\,\beta_{-1}\right] \rVert^2_2 \\
  &\equiv \underset{\beta\in\mathbb R^{p - 1}}{\text{arg min}}~\lVert Y - X \beta  \rVert^2_2.
\end{align*}

Therefore, if $\hat \beta = \left(\hat \beta_1,\, \hat\beta_{-1}^T\right)^T \in \mathbb R^p$ and 
\begin{equation*}
  \hat\beta_1 = \bar Y - \bar x^T \hat\beta_{-1},
\end{equation*}

then $\hat\beta$ also solves the uncentered problem
\begin{equation*}
  \hat \beta \equiv \left(\hat \beta_1,\, \hat\beta_{-1}^T\right)^T = \underset{\beta\in\mathbb R^p}{\text{arg min}}~\lVert Y - X \beta \rVert^2_2,
\end{equation*}

as desired.

# Question 2

## (a)

Define our objective function $f\,:\,\mathbb R^p \to \mathbb R$ by
\begin{align*}
  f(\beta) &= \lVert \tilde Y - \tilde X \beta \rVert^2_2 + \lambda \lVert\beta\rVert^2_2 \\
  &= \left( \tilde Y - \tilde X \beta \right)^T \left( \tilde Y - \tilde X \beta \right)^T + \lambda \beta^T \beta \\
  &= \tilde Y^T \tilde Y - \tilde Y^T \tilde X \beta - \beta^T \tilde X^T \tilde Y + \beta^T \tilde X^T \tilde X \beta + \lambda \beta^T \beta \\
  &= \tilde Y^T \tilde Y - 2\beta^T \tilde X^T \tilde Y + \beta^T \tilde X^T \tilde X \beta + \lambda \beta^T \beta.
\end{align*}

Therefore, by taking the gradient we find
\begin{equation*}
  \nabla f(\beta) = -2\tilde X^T \tilde Y + 2\tilde X^T \tilde X \beta + 2\lambda \beta,
\end{equation*}

as desired.

## (b)

The Hessian $\nabla^2 f(\beta)$ is given by
\begin{equation*}
  \nabla^2 f(\beta) = 2\tilde X^T \tilde X + 2\lambda \mathbb I_{p-1},
\end{equation*}

where $\mathbb I_{p-1}$ is the $(p-1)\times (p-1)$ identity matrix. Note that $2\tilde X^T \tilde X \in \mathbb S^{p-1}_+$ (positive semi-definite) and, for $\lambda > 0$, we have $2\lambda\mathbb I_{p - 1}\in \mathbb S^{p - 1}_{++}$ (positive definite). Therefore, for all nonzero vectors $v \in \mathbb R^{p - 1}$,
\begin{align*}
  v^T \nabla^2 f(\beta) v &= v^T \left( 2\tilde X^T \tilde X + 2\lambda \mathbb I_{p-1} \right) v \\
  &= 2 v^T \tilde X^T \tilde X v + 2\lambda v^T \mathbb I_{p - 1} v \\
  &= 2 \left( \underbrace{\lVert \tilde X v \rVert^2_2}_{\geq 0} +  \underbrace{\lambda \lVert v \rVert^2_2}_{>0 \text{ when } \lambda > 0} \right) \\
  &> 0.
\end{align*}

Hence,
\begin{equation*}
\nabla^2 f(\beta) = 2\tilde X^T \tilde X + 2\lambda \mathbb I_{p-1} \in \mathbb S^{p - 1}_{++},
\end{equation*}

and so $f$ must be strictly convex in $\beta$.

## (c)

Suppose a strictly convex function $f$ is globally minimized at distinct points $x$ and $y$. By strict convexity
\begin{equation*}
  \forall\,t\in (0, 1) \quad f(tx + (1-t)y) < tf(x) + (1-t)f(y).
\end{equation*}

Since $f$ is minimized at both $x$ and $y$ we have $f(x) = f(y)$, so
\begin{equation*}
  f(tx + (1-t)y) < tf(x) + (1-t)f(x) = f(x).
\end{equation*}

However, this implies that the point $z = tx + (1 - t)y$ yields a value of $f$ even *smaller* than at $x$, contradicting our assumption that $x$ is a global minimizer. Therefore, strict convexity implies that the global minimizer must be unique, and so for $\lambda > 0$, we are guaranteed that the above solution will be the unique solution to our penalized least squares problem.

## (d)

To write our function computing the ridge coefficients we first set $\nabla f(\beta) = 0$ 
\begin{equation*}
  \hat\beta^{(\lambda)}_{-1} = \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde Y.
\end{equation*}

For the purpose of computational efficiency we make use of the singular value decomposition of $\tilde X$
\begin{equation*}
  \tilde X = U D V^T,
\end{equation*}

for $U \in \mathbb R^{n\times n}$ and $V \in \mathbb R^{(p-1)\times (p-1)}$ both orthogonal matrices, $U^TU = \mathbb I_n$, $V^TV = \mathbb I_{p-1}$, and $D \in \mathbb R^{n\times (p-1)}$ a diagonal matrix with entries $\left\{d_{j}\right\}_{j = 1}^{\min(n,\,p-1)}$ along the main diagonal and zero elsewhere. Hence,
\begin{align*}
  \hat\beta^{(\lambda)}_{-1} &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde Y \\
  &= \left(\left(UDV^T\right)^T UDV^T + \lambda V V^T\right)^{-1} \left(UDV^T\right)^T \tilde Y \\
  &= \left(VD^TU^T UDV^T + \lambda V V^T\right)^{-1} VD^TU^T \tilde Y \\
  &= \left(V\left(D^TD + \lambda \mathbb I_{p- 1} \right)V^T\right)^{-1} VD^TU^T \tilde Y \\
  &= V\left(D^TD + \lambda \mathbb I_{p- 1} \right)^{-1} V^T VD^TU^T \tilde Y \\
  &= V\left(D^TD + \lambda \mathbb I_{p- 1} \right)^{-1} D^TU^T \tilde Y.
\end{align*}

Note that $D^TD + \lambda \mathbb I_{p- 1}$ is a diagonal $(p-1)\times(p-1)$ matrix with entries $d_j^2 + \lambda$, $j = 1, ..., p - 1$, and so the inverse $\left(D^TD + \lambda \mathbb I_{p- 1}\right)^{-1}$ must also be diagonal with entries $\left(d^2_j + \lambda\right)^{-1}$, $j = 1, ..., p - 1$. We exploit this to avoid performing a matrix inversion in our function. For brevity, let
\begin{equation*}
  D^* = \left(D^T D + \lambda I_{p - 1}\right)^{-1} D^T,
\end{equation*}

so that
\begin{equation*}
  \hat\beta^{(\lambda)} = V D^* U^T \tilde Y.
\end{equation*}

We present a function written in `R` performing such calculations below.

```{r}
ridge_coef <- function(X, y, lam) {
  Xm1 <- X[,-1] # remove leading column of 1's marking the intercept
  
  ytilde <- y - mean(y) # center response
  xbar <- colMeans(Xm1) # find predictor means
  # center each predictor according to its mean
  Xtilde <- Xm1 - tcrossprod(rep(1, nrow(Xm1)), xbar) 
  
  # compute the SVD on the centered design matrix
  Xtilde_svd <- svd(Xtilde)
  U <- Xtilde_svd$u
  d <- Xtilde_svd$d
  V <- Xtilde_svd$v
  
  # compute the inverse (D^T D + lambda I_{p-1})^{-1} D^T
  Dstar <- diag(d/(d^2 + lam))
  
  # compute ridge coefficients
  b <- V %*% (Dstar %*% crossprod(U, ytilde)) # slopes
  b1 <- mean(y) - crossprod(xbar, b) # intercept
  list(b1 = b1, b = b)
}
```

It turns out the following implementation will be considerably quicker
```{r}
scale_faster <- function(x,
                       center = TRUE,
                       scale = TRUE,
                       add_attr = TRUE,
                       rows = NULL,
                       cols = NULL) {
  # adapted from
  # https://www.r-bloggers.com/a-faster-scale-function/
  if (!is.null(rows) && !is.null(cols)) {
    x <- x[rows, cols, drop = FALSE]
  } else if (!is.null(rows)) {
    x <- x[rows, , drop = FALSE]
  } else if (!is.null(cols)) {
    x <- x[, cols, drop = FALSE]
  }
  # Get the column means
  cm <- colMeans(x, na.rm = TRUE)
  # Get the column sd
  if (scale) {
    csd <- colSds(x, center = cm)
  } else {
    # just divide by 1 if not
    csd <- rep(1, length = length(cm))
  }
  if (!center) {
    # just subtract 0
    cm <- rep(0, length = length(cm))
  }
  x <- t((t(x) - cm) / csd)
  if (add_attr) {
    if (center) {
      attr(x, "scaled:center") <- cm
    }
    if (scale) {
      attr(x, "scaled:scale") <- csd
    }
  }
  return(x)
}

ridge_coef_faster <- function(X, y, lam) {
  # Commented-out scaling parameters represent the transformations
  # used to make the output identical to that of 
  # coef(MASS::lm.ridge(X, y, lam))
  
  Xm1 <- X[,-1]
  n <- nrow(X); ybar <- .Internal(mean(y)); #sqrt_sc <- sqrt(n/(n - 1))
  #Xtilde <- scale_faster(Xm1) * sqrt_sc
  Xtilde <- scale_faster(Xm1, scale = F)
  ytilde <- y - ybar
  
  b <- chol2inv(chol(crossprod(Xtilde) + lam * diag(ncol(Xm1)))) %*% 
    crossprod(Xtilde, ytilde) #* 
    #sqrt_sc * 1/attr(Xtilde, "scaled:scale")

  b1 <- ybar - sum(attr(Xtilde, "scaled:center") * b)
  list(b1 = b1, b = b)
}
```

## (e)

We first take the expectation of $\hat\beta^{(\lambda)}_{-1}$
\begin{align*}
  \mathbb E \left[ \hat\beta^{(\lambda)}_{-1} \right] &= \mathbb E \left[ \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde Y \right] \\
  &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \mathbb E \left[ \tilde Y \right] \\
  &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde X \beta_{-1} 
\end{align*}

If $p >> n$ then using the SVD on $\tilde X$ may yield some speed improvements, that is, with $\tilde X = U D V^T$ as above, we find
\begin{align*}
  \mathbb E \left[ \hat\beta^{(\lambda)}_{-1} \right] &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde X \beta_{-1}  \\
  &= V \left(D^T D + \lambda \mathbb I_{p - 1}\right)^{-1} D^T D V^T \beta_{-1} \\
  &= V D^* V^T \beta_{-1}
\end{align*}

where $D^*$ is a diagonal $\min \left(n, p - 1\right) \times \min \left(n, p - 1\right)$ matrix with diagonal entries $\left\{ \frac{d^2_j}{d^2_j + \lambda} \right\}^{\min \left(n, p - 1\right)}_{j = 1}$ and zero elsewhere. We next compute the variance of our centered ridge estimates
\begin{align*}
  \text{Var} \left( \hat\beta^{(\lambda)}_{-1} \right) &= \text{Var} \left( \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde Y \right) \\
  &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \text{Var} \left( \tilde Y \right) \left( \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \right)^T \\
  &= \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \text{Var} \left( \tilde Y \right) \tilde X \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \\
  &= \sigma^2_* \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1} \tilde X^T \tilde X \left(\tilde X^T \tilde X + \lambda \mathbb I_{p - 1}\right)^{-1}
\end{align*}

as desired. We once again may be interested in applying the SVD on $\tilde X$ as we had done before. Such a decomposition gives us a more concise solution
\begin{equation*}
  \text{Var} \left( \hat\beta^{(\lambda)}_{-1} \right) = V D^{**} V^T
\end{equation*}

where $D^{**}$ is a diagonal $\min\left(n, p - 1\right)\times\min\left(n, p - 1\right)$ matrix with diagonal entries $\left\{ \frac{d_j^2}{\left(d_j^2 + \lambda\right)^2}\right\}^{\min\left(n, p - 1\right)}_{j = 1}$ and zero elsewhere. 

We now wish to perform a simulation study to estimate our theoretical values $\mathbb E \left[ \hat\beta^{(\lambda)}_{-1} \right]$ and $\text{Var} \left( \hat\beta^{(\lambda)}_{-1} \right)$. For readability we first define functions computing the theoretical mean and variance according to our above expressions.
```{r}
ridge_coef_params <- function(X, lam, beta, sigma) {
  n <- nrow(X); p <- ncol(X)
  betam1 <- beta[-1] # remove intercept term
  Xm1 <- X[,-1] # remove leading column of 1's in our design matrix
  
  xbar <- colMeans(Xm1) # find prector means
  # center each predictor according to its mean
  Xtilde <- sweep(Xm1, 2, xbar) 
  
  if (n >= p) {
    I <- diag(p - 1)
    inv <- solve(crossprod(Xtilde) + lam * I)
    
    b <- solve(crossprod(Xtilde) + lam * I) %*% (crossprod(Xtilde) %*% betam1)
    vcv <- sigma^2 * inv %*% crossprod(Xtilde) %*% inv
    list(b = b, vcv = vcv)
    
  } else {
    # compute SVD on the centered design matrix
    Xtilde_svd <- svd(Xtilde)
    d <- Xtilde_svd$d
    V <- Xtilde_svd$v
    
    Dstar <- diag(d^2/(d^2 + lam))
    Dstar2 <- diag(d^2/(d^2 + lam)^2)
    
    b <- V %*% (Dstar %*% crossprod(V, betam1))
    vcv <- V %*% tcrossprod(Dstar2, V)
    list(b = b, vcv = vcv)
  }
}
```

We may now perform our simulation.
```{r}
set.seed(124)

# set parameters
nsims <- 1e3
n <- 25
p <- 7
lam <- 4
beta_star <- 1:p
sigma_star <- 1

# generate fixed design matrix
X <- cbind(1, matrix(rnorm(n * (p - 1)), nrow = n))

# compute theoretical mean and variance
par_true <- ridge_coef_params(X, lam, beta_star, sigma_star)
b_true <- as.vector(par_true$b)
vcv_true <- par_true$vcv

# simulate ridge coefficients nsims times
# outputs a matrix with rows corresponding to coefficients
# and columns correspond to simulation number
b_hat <- replicate(nsims, {
  y <- X %*% beta_star + rnorm(n, 0, sigma_star)
  as.vector(ridge_coef_faster(X, y, lam)$b)
})

# estimate variance of b1, ..., b_p estimates
vcv_hat <- var(t(b_hat)) 

# print estimated fused ridge coefficients vs. expected values
b <- rbind(rowMeans(b_hat), b_true)
rownames(b) <- c("b_hat", "b_true")
round(b, 4)

# print absolute error between estimated and true fused ridge variances
round(abs(vcv_true - vcv_hat), 4) 
```

We see that the empirical sample estimates are very close to their theoretical values, as expected.

# Question 3

Prior to writing our cross-validation function we create some helper functions for the sake of readability
```{r}
ridge_cv_lam <- function(X, y, lam, K) {
  # Helper function for ridge_cv()
  # perform K-fold cross-validation on the ridge regression 
  # estimation problem over a single tuning parameter lam
  n <- nrow(X)
  
  if (K > n) { 
    stop(paste0("K > ", n, "."))
  } else if (K < 2) {
    stop("K < 2.")
  }
  
  # groups to cross-validate over
  folds <- cut(1:n, breaks = K, labels = F)
  # get indices of training subset
  train_idxs <- lapply(1:K, function(i) !(folds %in% i))
  
  cv_err <- sapply(train_idxs, function(tr_idx) {
    # train our model, extract fitted coefficients
    b_train <- unlist(ridge_coef_faster(X[tr_idx,], y[tr_idx], lam))
    
    # fit testing data
    yhat <- X[!tr_idx,] %*% b_train
    # compute test error
    sum((y[!tr_idx] - yhat)^2)
  })
  # weighted average (according to group size, some groups may have
  # +/- 1 member depending on whether sizes divided unevenly) of
  # cross validation error for a fixed lambda
  sum((cv_err * tabulate(folds)))/n
}
```

Then, our cross-validation function is as follows:
```{r}
ridge_cv <- function(X, y, lam.vec, K) {
  # perform K-fold cross-validation on the ridge regression 
  # estimation problem over tuning parameters given in lam.vec
  n <- nrow(X); p <- ncol(X)
  
  cv.error <- sapply(lam.vec, function(l) ridge_cv_lam(X, y, l, K))
  
  # extract best tuning parameter and corresponding coefficient estimates
  best.lam <- lam.vec[cv.error == min(cv.error)]
  best.fit <- ridge_coef_faster(X, y, best.lam)
  b1 <- best.fit$b1
  b <- best.fit$b
  
  list(b1 = b1, b = b, best.lam = best.lam, cv.error = cv.error)
}
```

# Question 4

For this problem we first set some global libraries/functions
```{r, message = F}
library(doParallel)

rmvn <- function(n, p, mu = 0, S = diag(p)) {
  # generates n (potentially correlated) p-dimensional normal deviates
  # given mean vector mu and variance-covariance matrix S
  # NOTE: S must be a positive-semidefinite matrix
  Z <- matrix(rnorm(n * p), nrow = n, ncol = p) # generate iid normal deviates
  C <- chol(S)
  mu + Z %*% C # compute our correlated deviates
}
loss1 <- function(beta, b) sum((b - beta)^2, na.rm = T)
loss2 <- function(X, beta, b) sum((X %*% (beta - b))^2, na.rm = T)
```

and global parameters which remain constant across (a)-(d)
```{r}
set.seed(124)

# global parameters
nsims <- 4
n <- 100
Ks <- c(5, 10, n)
lams <- 10^seq(-8, 8, 0.5)
sigma_star <- sqrt(1/2)

# empty data structure to store our results
coef_list <- vector(mode = 'list', length = length(Ks) + 1)
names(coef_list) <- c("OLS", "K5", "K10", "Kn")
```

## (a)

```{r, cache = T}
# set parameters
p <- 50
theta <- 0.5

# generate data
beta_star <- rnorm(p, 0, sigma_star)
SIGMA <- outer(1:(p - 1), 1:(p - 1), FUN = function(a, b) theta^abs(a - b))
X <- cbind(1, rmvn(n, p - 1, 0, SIGMA))

# simulation
pt <- proc.time()
registerDoParallel(cores = 4)

sim <- foreach(1:nsims, .combine = cbind) %dopar% {
  y <- X %*% beta_star + rnorm(n, 0, sigma_star)
  
  # compute OLS estimates (lambda = 0)
  ols_fit <- ridge_coef_faster(X, y, 0)
  coef_list[[1]] <- c(ols_fit$b1, ols_fit$b)
  
  # compute the cross-validated ridge estimates for each K
  coef_list[2:(length(Ks) + 1)] <- sapply(Ks, function(k) {
    rcv <- ridge_cv(X, y, lam.vec = lams, K = k)
    list(coefs = c(rcv$b1, rcv$b))
  })

  l1 <- sapply(coef_list, function(b) loss1(beta_star, b))
  l2 <- sapply(coef_list, function(b) loss2(X, beta_star, b))
  list(l1, l2)
}
sim_loss <- lapply(1:nrow(sim), function(i) sapply(sim[i,], function(s) s))
names(sim_loss) <- c("Loss 1", "Loss 2")

sim_means <- t(sapply(sim_loss, function(s) rowMeans(s)))
sim_se <- t(
  sapply(sim_loss, function(s) apply(s, 1, function(x) sd(x)/sqrt(length(x)))))
proc.time() - pt

# report results
round(sim_means, 4)
round(sim_se, 4)
```

## (b)

```{r, cache = T}
# set parameters
p <- 50
theta <- 0.9

# generate data
beta_star <- rnorm(p, 0, sigma_star)
SIGMA <- outer(1:(p - 1), 1:(p - 1), FUN = function(a, b) theta^abs(a - b))
X <- cbind(1, rmvn(n, p - 1, 0, SIGMA))

# simulation
pt <- proc.time()
registerDoParallel(cores = 4)

sim <- foreach(1:nsims, .combine = cbind) %dopar% {
  y <- X %*% beta_star + rnorm(n, 0, sigma_star)

  # compute OLS estimates (lambda = 0)  
  ols_fit <- ridge_coef_faster(X, y, 0)
  coef_list[[1]] <- c(ols_fit$b1, ols_fit$b)

  # compute the cross-validated ridge estimates for each K
  coef_list[2:(length(Ks) + 1)] <- sapply(Ks, function(k) {
    rcv <- ridge_cv(X, y, lam.vec = lams, K = k)
    list(coefs = c(rcv$b1, rcv$b))
  })

  l1 <- sapply(coef_list, function(b) loss1(beta_star, b))
  l2 <- sapply(coef_list, function(b) loss2(X, beta_star, b))
  list(l1, l2)
}
sim_loss <- lapply(1:nrow(sim), function(i) sapply(sim[i,], function(s) s))
names(sim_loss) <- c("Loss 1", "Loss 2")

sim_means <- t(sapply(sim_loss, function(s) rowMeans(s)))
sim_se <- t(
  sapply(sim_loss, function(s) apply(s, 1, function(x) sd(x)/sqrt(length(x)))))
proc.time() - pt

# report results
round(sim_means, 4)
round(sim_se, 4)
```

## (c)

```{r, cache = T}
# set parameters
p <- 200
theta <- 0.5

# generate data
beta_star <- rnorm(p, 0, sigma_star)
SIGMA <- outer(1:(p - 1), 1:(p - 1), FUN = function(a, b) theta^abs(a - b))
X <- cbind(1, rmvn(n, p - 1, 0, SIGMA))

# simulation
pt <- proc.time()
registerDoParallel(cores = 4)

sim <- foreach(1:nsims, .combine = cbind) %dopar% {
  y <- X %*% beta_star + rnorm(n, 0, sigma_star)

  # compute OLS estimates (lambda = 0)  
  ols_fit <- ridge_coef(X, y, 0)
  coef_list[[1]] <- c(ols_fit$b1, ols_fit$b)

  # compute the cross-validated ridge estimates for each K
  coef_list[2:(length(Ks) + 1)] <- sapply(Ks, function(k) {
    rcv <- ridge_cv(X, y, lam.vec = lams, K = k)
    list(coefs = c(rcv$b1, rcv$b))
  })

  l1 <- sapply(coef_list, function(b) loss1(beta_star, b))
  l2 <- sapply(coef_list, function(b) loss2(X, beta_star, b))
  list(l1, l2)
}
sim_loss <- lapply(1:nrow(sim), function(i) sapply(sim[i,], function(s) s))
names(sim_loss) <- c("Loss 1", "Loss 2")

sim_means <- t(sapply(sim_loss, function(s) rowMeans(s)))
sim_se <- t(
  sapply(sim_loss, function(s) apply(s, 1, function(x) sd(x)/sqrt(length(x)))))
proc.time() - pt

# report results
round(sim_means, 4)
round(sim_se, 4)
```

## (d)

```{r, cache = T}
# set parameters
p <- 200
theta <- 0.9

# generate data
beta_star <- rnorm(p, 0, sigma_star)
SIGMA <- outer(1:(p - 1), 1:(p - 1), FUN = function(a, b) theta^abs(a - b))
X <- cbind(1, rmvn(n, p - 1, 0, SIGMA))

# simulation
pt <- proc.time()
registerDoParallel(cores = 4)

sim <- foreach(1:nsims, .combine = cbind) %dopar% {
  y <- X %*% beta_star + rnorm(n, 0, sigma_star)
  
  # compute OLS estimates (lambda = 0)  
  ols_fit <- ridge_coef(X, y, 0)
  coef_list[[1]] <- c(ols_fit$b1, ols_fit$b)

  # compute the cross-validated ridge estimates for each K
  coef_list[2:(length(Ks) + 1)] <- sapply(Ks, function(k) {
    rcv <- ridge_cv(X, y, lam.vec = lams, K = k)
    list(coefs = c(rcv$b1, rcv$b))
  })

  l1 <- sapply(coef_list, function(b) loss1(beta_star, b))
  l2 <- sapply(coef_list, function(b) loss2(X, beta_star, b))
  list(l1, l2)
}
sim_loss <- lapply(1:nrow(sim), function(i) sapply(sim[i,], function(s) s))
names(sim_loss) <- c("Loss 1", "Loss 2")

sim_means <- t(sapply(sim_loss, function(s) rowMeans(s)))
sim_se <- t(
  sapply(sim_loss, function(s) apply(s, 1, function(x) sd(x)/sqrt(length(x)))))
proc.time() - pt

# report results
round(sim_means, 4)
round(sim_se, 4)
```

# Question 5

## (a)

Taking the gradient of our objective function $g$ with respect to coefficient vector $\beta$ yields
\begin{align*}
  \nabla_\beta g(\beta,\sigma^2) &= \nabla_\beta \left( \frac{n}{2} \log \sigma^2 + \frac{1}{2\sigma^2} \lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda}{2} \lVert \beta \rVert^2_2 \right) \\
  &= \frac{1}{\sigma^2} \left( -\tilde X^T \tilde Y + \tilde X^T \tilde X \beta \right) + \lambda \beta,
\end{align*}

while the gradient of $g$ with respect to $\sigma^2$ is given by
\begin{align*}
  \nabla_{\sigma^2} g(\beta,\sigma^2) &= \nabla_\beta \left( \frac{n}{2} \log \sigma^2 + \frac{1}{2\sigma^2} \lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda}{2} \lVert \beta \rVert^2_2 \right) \\
  &= \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde X \beta \rVert^2_2.
\end{align*}

as desired.

## (b)

We first consider the objective function in terms of $\beta$. We find the Hessian with respect to $\beta$
\begin{align*}
  \nabla^2_{\beta} g\left(\beta,\sigma^2\right) &= \nabla^2_{\beta} \left( \frac{n}{2} \log \sigma^2+ \frac{1}{2\sigma^2} \lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda}{2} \lVert \beta \rVert^2_2 \right) \\
  &= \nabla_{\beta} \left( \frac{1}{\sigma^2} \tilde X^T \left( -\tilde Y + \tilde X \beta \right) + \lambda \beta \right) \\
  &= \tilde X^T \tilde X + \lambda \mathbb I_{p - 1}.
\end{align*}

The symmetric matrix $\tilde X^T \tilde X$ is always positive semi-definite, and for $\lambda \geq 0$, $\lambda \mathbb I_{p - 1}$ will also be positive semi-definite (and strictly positive definite when $\lambda > 0$). Thus, the Hessian with respect to $\beta$ must be positive semi-definite
\begin{equation*}
  \nabla^2_{\beta} g\left(\beta,\sigma^2\right) = \tilde X^T \tilde X + \lambda \mathbb I_{p - 1} \in \mathbb S^{p - 1}_+,
\end{equation*}

and so our objective function $g \left(\beta, \sigma^2\right)$ is convex in $\beta$. Now, considering the Hessian with respect to $\sigma^2$,

\begin{align*}
  \nabla^2_{\sigma^2} g\left(\beta,\sigma^2\right) &= \nabla^2_{\sigma^2} \left( \frac{n}{2} \log \sigma^2+ \frac{1}{2\sigma^2} \lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda}{2} \lVert \beta \rVert^2_2 \right) \\
  &= \nabla_{\sigma^2} \left( \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde X \beta \rVert^2_2 \right) \\
  &= -\frac{n}{2\sigma^4} + \frac{1}{\sigma^6} \lVert \tilde Y - \tilde X \beta \rVert^2_2.
\end{align*}

For $g$ to be convex in $\sigma^2$ we require $\nabla^2_{\sigma^2} g\left(\beta,\sigma^2\right) \geq 0$. However, such a condition is equivalent to
\begin{equation*}
  n \geq \frac{2}{\sigma^2} \lVert \tilde Y - \tilde X \beta \rVert^2_2.
\end{equation*}

As a counterexample consider the following data
```{r}
set.seed(124)
n <- 20
p <- 100
beta <- rep(0.1, p)
sigma <- sqrt(2)

Xtilde <- matrix(rnorm(n * p), nrow = n)
eps <- rnorm(n, 0, sigma^2)
ytilde <- Xtilde %*% beta + eps

rhs <- as.numeric(2/sigma^2 * crossprod(ytilde - Xtilde %*% beta))
rhs
n >= rhs
```

and so it is not the case that $\nabla^2_{\sigma^2} g\left(\beta,\sigma^2\right)$ is (always) nonnegative, implying that our objective function $g\left(\beta,\sigma^2\right)$ is *not* convex in $\sigma^2$.

## (c)

Let $\bar\beta$ be a solution to our maximum likelihood ridge estimation problem such that, for $\lambda > 0$, we have
\begin{equation*}
  \tilde Y - \tilde X \bar \beta = 0.
\end{equation*}

Since $\bar\beta$ is a solution it must satisfy our first order condition
\begin{equation*}
 \nabla_\beta g(\beta,\sigma^2) = \frac{1}{\sigma^2} \left( -\tilde X^T \tilde Y +  \tilde X^T \tilde X \beta \right) + \lambda \beta = 0 \iff \frac{1}{\sigma^2} \left( \tilde X^T \left( - \tilde Y + \tilde X \beta \right) \right) + \lambda \beta = 0.
\end{equation*}

Thus, for such a solution $\bar\beta$ and $\lambda > 0$,
\begin{align*}
  0 &= \frac{1}{\sigma^2} \left( \tilde X^T \left( -\tilde Y + \tilde X \bar\beta \right) \right) + \lambda \bar\beta \\
  &= \frac{1}{\sigma^2} \left( \tilde X^T \left( -\tilde Y + \tilde Y \right) \right) + \lambda \bar\beta \\
  &= \lambda \bar\beta \\
  \iff \bar\beta &= 0.
\end{align*}

Similarly, using our second first order condition $\nabla_{\sigma^2} g(\beta,\sigma^2) = 0$, at $\beta = \bar\beta$,
\begin{align*}
  \nabla_{\sigma^2} g(\beta,\sigma^2) &= \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde X \beta \rVert^2_2 \\
  &= \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde X \bar\beta \rVert^2_2 \\
  &= \frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde Y \rVert^2_2 \\
  &= \frac{n}{2\sigma^2} = 0 
\end{align*}  

This conditions implies that either $n = 0$ or $\sigma^2 \to \infty$. Thus, no such global minimizer could exist.

## (d)

Solving our first order conditions
\begin{align*}
\frac{1}{\sigma^2} \left( \tilde X^T \left( -\tilde Y + \tilde X \bar\beta \right) \right) + \lambda \bar\beta &= 0 \\
\frac{n}{2\sigma^2} - \frac{1}{2\sigma^4} \lVert \tilde Y - \tilde X \beta \rVert^2_2 &= 0,
\end{align*}

we find the maximum likelihood estimate $\hat\beta^{\left(\lambda,\, ML\right)}$ to be
\begin{equation*}
  \hat\beta^{\left(\lambda,\, ML\right)} = \left(\tilde X^T \tilde X + \sigma^2 \lambda \mathbb I_{p - 1} \right)^{-1} \tilde X^T \tilde Y.
\end{equation*}

and the maximum likelihood estimate $\hat\sigma^{2\,\left(\lambda,\, ML\right)}$ to be
\begin{equation*}
  \hat\sigma^{2\,\left(\lambda,\, ML\right)} = \frac{1}{n} \lVert \tilde Y - \tilde X \hat\beta^{\left(\lambda,\, ML\right)} \rVert^2_2
\end{equation*}

To compute such estimates we may use the following algorithm: Consider some fixed data set $\mathcal D = \left\{X, Y\right\}$ and a fixed tuning parameter $\lambda$.

(1) Center the data: Center each predictor by its mean $X \mapsto \tilde X$, center the response vector by its mean $Y \mapsto \tilde Y$.

(2) Have some initial proposal for the estimate $\hat\sigma^{2\,\left(\lambda,\, ML\right)}_0 \in \mathbb R^+$.

(3) Compute an initial proposal for $\hat\beta^{\left(\lambda,\, ML\right)}_0$ based on $\hat\sigma^{2\,\left(\lambda,\, ML\right)}_0$.

(4) Update our variance estimate $\hat\sigma^{2\,\left(\lambda,\, ML\right)}_i$ using the previous estimate of $\hat\beta^{\left(\lambda,\, ML\right)}_{i - 1}$.

(5) Update our coefficient estimate $\hat\beta^{\left(\lambda,\, ML\right)}_i$ using the new estimate of $\hat\sigma^{2\,\left(\lambda,\, ML\right)}_i$.

(6) Repeat steps (5)-(6) until some convergence criteria is met, say $\lVert \hat\sigma^{2\,\left(\lambda,\, ML\right)}_i - \hat\sigma^{2\,\left(\lambda,\, ML\right)}_{i - 1} \rVert$, is small.

## (e)

Our function is as follows

```{r}
ridge_coef_mle <- function(X, y, lam, tol = 1e-16) {
  Xm1 <- X[,-1] # remove leading column of 1's marking the intercept
  
  ytilde <- y - mean(y) # center response
  xbar <- colMeans(Xm1) # find predictor means
  Xtilde <- sweep(Xm1, 2, xbar) # center each predictor according to its mean
  
  # compute the SVD on the centered design matrix
  Xtilde_svd <- svd(Xtilde)
  U <- Xtilde_svd$u
  d <- Xtilde_svd$d
  V <- Xtilde_svd$v
  
  ## generate some initial guess for sigma and beta
  sig0 <- rexp(1)
  Dstar <- diag(d/(d^2 + sig0^2 * lam))
  b0 <- V %*% (Dstar %*% crossprod(U, ytilde)) 
  
  i <- 1
  repeat {
    # update sigma and beta
    sig_new <- sqrt(1/n * crossprod(ytilde - Xtilde %*% b0))
    Dstar <- diag(d/(d^2 + sig_new^2 * lam))
    b_new <- V %*% (Dstar %*% crossprod(U, ytilde))
    
    if (abs(sig_new^2 - sig0^2) < tol)
      break
    
    sig0 <- sig_new
    b0 <- b_new
    i <- i + 1
  }
  list(niter = i, sigma = as.numeric(sig_new), b = b_new)
} 
grad_mle <- function(X, y, lam, b, s) {
  n <- nrow(X)
  Xm1 <- X[,-1] # remove leading column of 1's marking the intercept
  ytilde <- y - mean(y) # center response
  xbar <- colMeans(Xm1) # find predictor means
  Xtilde <- sweep(Xm1, 2, xbar) # center each predictor according to its mean
  
  gb <- 1/s^2 * crossprod(Xtilde, Xtilde %*% b - ytilde) + lam * b
  gs <- n/(2 * s^2)  - 1/(2 * s^4) * crossprod(ytilde - Xtilde %*% b)
  c(grad_b = gb, grad_s = gs)
}
```

## (f)

```{r}
set.seed(124)
n <- 100
p <- 5
lam <- 1
beta_star <- (-1)^(1:p) * rep(5, p)
sigma_star <- sqrt(1/2)

X <- cbind(1, matrix(rnorm(n * (p - 1)), nrow = n))
y <- X %*% beta_star + rnorm(n, 0, sigma_star)

rcm <- ridge_coef_mle(X, y, lam)
rcm

grad_mle(X, y, lam, rcm$b, rcm$sigma)
```

as desired.

# Question 6

## (a)

Consider our objective function
\begin{equation*}
  f(\beta) = \frac{1}{2}\lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda_1}{2}\lVert \beta\rVert^2_2 + \frac{\lambda_2}{2} \sum^p_{j = 2} \left(\beta_j - \beta_{j - 1}\right)^2
\end{equation*}

To show convexity we wish to show $\nabla^2 f(\beta) \in \mathbb S^{p-1}_+$. However, it's not immediately obvious how to take such a gradient with our fused sum terms $\left(b_j - \beta_{j-1}\right)^2$. One way to get around this is to define vector $B \in \mathbb R^{p-1}$ given by
\begin{equation*}
  B =
  \begin{bmatrix}
    \beta_2 - \beta_1 \\
    \vdots \\
    \beta_p - \beta_{p-1}
  \end{bmatrix}
\end{equation*}

Then
\begin{equation*}
  \sum^p_{j = 2}\left(\beta_j - \beta_{j - 1}\right)^2 = B^T B 
\end{equation*}

In order to achieve our task of expressing the fused sum in terms of the vector $\beta$ we must next decompose $B$ into a product of $\beta$ and some matrix. To this end we define matrix $A \in \mathbb R^{(p-2)\times (p-1)}$ with entries -1 along the main diagonal and 1 along the upper diagonal, i.e.,
\begin{equation*}
  A = 
  \begin{bmatrix}
    -1 & 1 & 0 & \cdots & 0 & 0 \\
    0 & -1 & 1 & \cdots & 0 & 0 \\
    \vdots & \vdots & \vdots & \ddots & \vdots & \vdots \\
    0 & 0 & 0 & \cdots & -1 & 1 
  \end{bmatrix}
\end{equation*}

Then
\begin{align*}
  \sum^p_{j = 2} \left(\beta_j - \beta_{j - 1}\right)^2 &= B^T B \\
  &= \beta^T A^T A \beta \\
  &\equiv \lVert A\beta\rVert^2_2
\end{align*}

Therefore, our objective function can be expressed as
\begin{align*}
  f(\beta) &= \frac{1}{2}\lVert \tilde Y - \tilde X \beta \rVert^2_2 + \frac{\lambda_1}{2}\lVert \beta \rVert^2_2 + \frac{\lambda_2}{2}\lVert A \beta \rVert^2_2 \\
  &\equiv \frac{1}{2} \tilde Y^T \tilde Y - \beta^T \tilde X^T \tilde Y + \frac{1}{2} \beta^T \tilde X^T \tilde X \beta + \frac{\lambda_1}{2}\beta^T \beta + \frac{\lambda_2}{2}\beta^T A^T A \beta
\end{align*}

Hence
\begin{equation*}
  \nabla f(\beta) = -\tilde X^T \tilde Y + \tilde X^T \tilde X \beta + \lambda_1 \beta + \lambda_2 A^T A \beta 
\end{equation*}

admitting the Hessian
\begin{equation*}
  \nabla^2 f(\beta) = \tilde X^T \tilde X + \lambda_1 \mathbb I_{p-1} + \lambda_2 A^T A 
\end{equation*}

Recalling that a matrix multiplied with its transpose must always be positive semi-definite, we find $\tilde X^T X$ and $A^T A$ must be positive semi-definite. Thus, since $\lambda_1 > 0$, we find that our sum $\tilde X^T \tilde X + \lambda_1 \mathbb I_{p-1} + \lambda_2 A^T A = \nabla^2 f(\beta)$ is positive semi-definite, and so $f(\beta)$ must be strictly convex, as desired.

## (b)

We first solve for $\hat\beta^{(\lambda_1,\,\lambda_2)}_{-1}$ in (a) by setting $\nabla f(\beta) = 0$
\begin{align*}
  0 &= -\tilde X^T \tilde Y + \tilde X^T \tilde X \beta + \lambda_1 \beta + \lambda_2 A^T A \beta \\
  \tilde X^T \tilde Y &= \left( \tilde X^T \tilde X + \lambda_1 \mathbb I_{p - 1} + \lambda_2 A^T A \right) \beta \\
  \implies \hat\beta^{(\lambda_1,\,\lambda_2)}_{-1} &= M \tilde X^T \tilde Y
\end{align*}

where we have set $M = \left( \tilde X^T \tilde X + \lambda_1 \mathbb I_{p - 1} + \lambda_2 A^T A \right)^{-1}$ for brevity. Therefore
\begin{align*}
  \mathbb E \left[ \hat\beta^{(\lambda_1,\,\lambda_2)}_{-1} \right] &= \mathbb E \left[M \tilde X^T \tilde Y \right] \\
  &= M \tilde X^T \mathbb E \left[ \tilde Y \right] \\
  &= M \tilde X^T \beta_{*,\,-1}
\end{align*}

and
\begin{align*}
  \text{Var}\left( \hat\beta^{(\lambda_1,\,\lambda_2)}_{-1} \right) &= \text{Var}\left( M \tilde X^T Y \right) \\
  &= M \tilde X^T \text{Var} \left( \tilde Y \right) \tilde X M^T \\
  &= \sigma^2_* M \tilde X^T \tilde X M^T
\end{align*}

as desired. We now perform our fused ridge simulation study to test the theoretical values with some empirical estimates. We first define our fused ridge coefficient estimation function (as well as functions permitting us to easily compute the theoretical means and variances of the fused ridge problem)
```{r}
fused_ridge_coef <- function(X, y, lam1, lam2) {
  n <- nrow(X); p <- ncol(X)
  Xm1 <- X[,-1] # remove leading column of 1's marking the intercept
  
  ytilde <- y - mean(y) # center response
  xbar <- colMeans(Xm1) # find predictor means
  Xtilde <- sweep(Xm1, 2, xbar) # center each predictor according to its mean
  
  I <- diag(p - 1)
  UD <- cbind(rep(0, p - 2), diag(p - 2)) # upper diagonal matrix
  J <- -1 * cbind(diag(p - 2), rep(0, p - 2)) # diag (p - 2)*(p - 1) matrix
  A <- J + UD
  
  M <- solve(crossprod(Xtilde) + lam1 * I + lam2 * crossprod(A))
  b <- M %*% crossprod(Xtilde, y)
  b0 <- mean(y) - crossprod(xbar, b)
  return(list(b0 = b0, b = b))
}
fused_ridge_coef_params <- function(X, lam1, lam2, beta, sigma) {
  # omits intercept term b0
  # returns theoretical means and variances for the fused ridge problem
  n <- nrow(X); p <- ncol(X)
  Xm1 <- X[,-1] # remove leading column of 1's marking the intercept
  betam1 <- beta[-1] # remove intercept term
  
  xbar <- colMeans(Xm1) # find predictor means
  Xtilde <- sweep(Xm1, 2, xbar) # center each predictor according to its mean
  
  I <- diag(p - 1)
  UD <- cbind(rep(0, p - 2), diag(p - 2)) # upper diagonal matrix
  J <- -1 * cbind(diag(p - 2), rep(0, p - 2)) # diag (p - 2)*(p - 1) matrix
  A <- J + UD
  
  M <- solve(crossprod(Xtilde) + lam1 * I + lam2 * crossprod(A))
  b <- M %*% crossprod(Xtilde, (Xtilde %*% betam1))
  
  vcv <- matrix(0, nrow = p - 1, ncol = p - 1)
  if (n > p) { # when n > p this matrix multiplication routine is quicker
    vcv <- sigma^2 * M %*% tcrossprod(crossprod(Xtilde), M)
  } else { # when p > n this matrix multiplication routine is quicker
   vcv <- sigma^2 * tcrossprod(M, Xtilde) %*% tcrossprod(Xtilde, M)
  }
  
  list(b = b, vcv = vcv)
}
```

We now simulate some data to test our estimates:

```{r}
set.seed(124)

# set parameters
nsims <- 1e4
n <- 1e2
p <- 5
lam1 <- 1
lam2 <- 1
sigma_star <- 1
beta_star <- rnorm(p)

# generate (fixed) design matrix
X <- cbind(rep(1, n), matrix(rnorm(n * (p - 1)), nrow = n, ncol = p - 1))

# compute expected parameter values
par_true <- fused_ridge_coef_params(X, lam1, lam2, beta_star, sigma_star)
b_true <- as.vector(par_true$b)
vcv_true <- par_true$vcv

# simulate our fused ridge coefficients nsims times
# outputs a matrix with rows corresponding to coefficients
# and columns correspond to simulation number
pt <- proc.time()
b_hat <- replicate(nsims, { 
  y <- X %*% beta_star + rnorm(n, 0, sigma_star) # generate response
  as.vector(fused_ridge_coef(X, y, lam1, lam2)$b)
})
proc.time() - pt

# estimate variance of b2, ..., b_p estimates
vcv_hat <- var(t(b_hat)) 

# print estimated fused ridge coefficients vs. expected values
b <- rbind(rowMeans(b_hat), b_true)
rownames(b) <- c("b_hat", "b_true")
round(b, 4)

# print absolute error between estimated and true fused ridge variances
round(abs(vcv_true - vcv_hat), 4) 
```

As a case study, we may look at the simulations of $\hat\beta^{(\lambda_1,\,\lambda_2)}_2$ and compare it with it's theoretical distribution. Note that the estimates $\hat\beta^{(\lambda_1,\,\lambda_2)} = M\tilde X^T \tilde Y$ are normally distributed because they are a linear combination of $\tilde Y \sim \mathcal N(\tilde X\beta, \sigma^2)$ (when our noise terms $\epsilon \sim \mathcal N(0, \sigma^2)$). We visualize the histogram of the $\hat\beta^{(\lambda_1,\,\lambda_2)}_2$ simulations with its empirical and theoretical densities overlaid (dashed, solid), along with its expected value (vertical line) below.

```{r, fig.align = 'center', fig.height = 4, fig.width = 5.5, echo = F}
i <- 2

hist(b_hat[i,], breaks = 50, freq = F, 
     xlab = substitute(paste(hat(beta)[ix]), list(ix = i)),
     main = substitute(paste("Histogram of ", hat(beta)[ix], " Simulations"), 
     list(ix = i)))
abline(v = b_true[i], col = 'red', lwd = 2)

x <- seq(min(b_hat[i,]), max(b_hat[i,]), length.out = 1e3)
z <- dnorm(x, mean = b_true[i], sd = sqrt(vcv_true[i, i]))
lines(z ~ x, lwd = 2)
lines(density(b_hat[i,]), lwd = 2, lty = 'dashed')
```


<!-- \newpage -->
<!-- # Appendix -->

<!-- ## Computing $\mathbb E \left[ \hat\beta^{(\lambda)} \right]$ -->

<!-- Consider the case of $n >> p$ -->

<!-- ```{r} -->
<!-- library(microbenchmark) -->
<!-- set.seed(124) -->

<!-- #===== Large n case =====# -->
<!-- # parameters -->
<!-- n <- 1e2 -->
<!-- p <- 1e1 -->
<!-- lam <- 1 -->

<!-- # generate data -->
<!-- beta <- rnorm(p) -->
<!-- X <- matrix(rnorm(n * p), nrow = n) -->
<!-- I <- diag(p) -->

<!-- # define functions  -->
<!-- f1 <- function() solve(crossprod(X) + lam * I) %*% (crossprod(X) %*% beta) -->
<!-- f2 <- function() { -->
<!--   X_svd <- svd(X) -->
<!--   V <- X_svd$v -->
<!--   d <- X_svd$d -->
<!--   Dstar <- diag(d^2/(d^2 + lam)) -->
<!--   V %*% (Dstar %*% crossprod(V, beta)) -->
<!-- } -->

<!-- # test speed -->
<!-- microbenchmark(f1(), f2(), times = 1e3, unit = "us") -->
<!-- ``` -->

<!-- and the case for $p >> n$ -->
<!-- ```{r} -->
<!-- #===== Large p case =====# -->
<!-- # parameters -->
<!-- n <- 1e1 -->
<!-- p <- 1e2 -->
<!-- lam <- 1 -->

<!-- # generate data -->
<!-- beta <- rnorm(p) -->
<!-- X <- matrix(rnorm(n * p), nrow = n) -->
<!-- I <- diag(p) -->

<!-- # define functions  -->
<!-- f1 <- function() solve(crossprod(X) + lam * I) %*% (crossprod(X) %*% beta) -->
<!-- f2 <- function() { -->
<!--   X_svd <- svd(X) -->
<!--   V <- X_svd$v -->
<!--   d <- X_svd$d -->
<!--   Dstar <- diag(d^2/(d^2 + lam)) -->
<!--   V %*% (Dstar %*% crossprod(V, beta)) -->
<!-- } -->

<!-- # test speed -->
<!-- microbenchmark(f1(), f2(), times = 1e3, unit = "us") -->
<!-- ``` -->

<!-- and $n \approx p$ -->
<!-- ```{r} -->
<!-- #===== n ~ p case =====# -->
<!-- # parameters -->
<!-- n <- 1e2 -->
<!-- p <- 1e2 -->
<!-- lam <- 1 -->

<!-- # generate data -->
<!-- beta <- rnorm(p) -->
<!-- X <- matrix(rnorm(n * p), nrow = n) -->
<!-- I <- diag(p) -->

<!-- # define functions  -->
<!-- f1 <- function() solve(crossprod(X) + lam * I) %*% (crossprod(X) %*% beta) -->
<!-- f2 <- function() { -->
<!--   X_svd <- svd(X) -->
<!--   V <- X_svd$v -->
<!--   d <- X_svd$d -->
<!--   Dstar <- diag(d^2/(d^2 + lam)) -->
<!--   V %*% (Dstar %*% crossprod(V, beta)) -->
<!-- } -->

<!-- # test speed -->
<!-- microbenchmark(f1(), f2(), times = 1e3, unit = "us") -->
<!-- ``` -->


<!-- ## Matrix Multiplication Timing -->

<!-- Consider the following matrix multiplication benchmarks (for the cases of $n >> p$ and $p >> n$). -->

<!-- ```{r} -->
<!-- set.seed(124) -->
<!-- #===== Large n case =====# -->

<!-- # set parameters -->
<!-- n <- 1e3 -->
<!-- p <- 1e2 -->
<!-- lam <- 1 -->

<!-- # generate data -->
<!-- X <- matrix(rnorm(n * p), nrow = n) -->
<!-- beta <- rnorm(p) -->
<!-- eps <- rnorm(n) -->
<!-- y <- X %*% beta + eps -->

<!-- ytilde <- y - mean(y) -->
<!-- xbar <- colMeans(X) -->
<!-- Xtilde <- sweep(X, 2, xbar) -->

<!-- # compute decomposition -->
<!-- Xtilde_svd <- svd(Xtilde) -->
<!-- U <- Xtilde_svd$u -->
<!-- d <- Xtilde_svd$d -->
<!-- V <- Xtilde_svd$v -->
<!-- Dstar <- diag(d/(d^2 + lam)) -->

<!-- # define multiplication functions -->
<!-- f1 <- function() V %*% Dstar %*% t(U) %*% ytilde -->
<!-- f2 <- function() V %*% Dstar %*% (t(U) %*% ytilde) -->
<!-- f3 <- function() V %*% (Dstar %*% (t(U) %*% ytilde)) -->
<!-- f4 <- function() V %*% (Dstar %*% crossprod(U, ytilde)) -->
<!-- f5 <- function() V %*% crossprod(Dstar, crossprod(U, ytilde)) -->

<!-- # test speed -->
<!-- microbenchmark(f1(), f2(), f3(), f4(), f5(), times = 100, unit = "us") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #===== Large p case =====# -->
<!-- set.seed(124) -->

<!-- # set parameters -->
<!-- n <- 1e2 -->
<!-- p <- 1e3 -->
<!-- lam <- 1 -->

<!-- # generate data -->
<!-- X <- matrix(rnorm(n * p), nrow = n) -->
<!-- beta <- rnorm(p) -->
<!-- eps <- rnorm(n) -->
<!-- y <- X %*% beta + eps -->

<!-- # define multiplication functions -->
<!-- f1 <- function() V %*% Dstar %*% t(U) %*% ytilde -->
<!-- f2 <- function() V %*% Dstar %*% (t(U) %*% ytilde) -->
<!-- f3 <- function() V %*% (Dstar %*% (t(U) %*% ytilde)) -->
<!-- f4 <- function() V %*% (Dstar %*% crossprod(U, ytilde)) -->
<!-- f5 <- function() V %*% crossprod(Dstar, crossprod(U, ytilde)) -->

<!-- # test speed -->
<!-- microbenchmark(f1(), f2(), f3(), f4(), f5(), times = 100, unit = "us") -->
<!-- ``` -->


